{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = code_example = \\\n",
    "\"\"\"# Handshake successful, kill previous client if there is any.\n",
    "with current_client_pid.get_lock():\n",
    "    old_pid = current_client_pid.value\n",
    "    if old_pid != 0:\n",
    "        print(f\"Booting previous client (pid={old_pid})\")\n",
    "        os.kill(old_pid, signal.SIGKILL)\n",
    "        current_client_pid.value = os.getpid()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def get_src_tokens_and_logits(in_text: str, model_name: str, device: str = None) -> Tuple[List[str], Tensor]:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    print('Model: ', model.__class__.__name__)\n",
    "    print('Adding bos token...')\n",
    "    in_text = f'{tokenizer.bos_token}{in_text}'\n",
    "    print('Input:')\n",
    "    print(in_text)\n",
    "    print('------')\n",
    "    inputs: Tensor = tokenizer(in_text, return_tensors='pt').data['input_ids'].squeeze()\n",
    "    print(\"Input token ids' shape: \", inputs.shape)\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('Device: ', device)\n",
    "    outputs: Tensor = model(inputs.to(torch.device(device)))\n",
    "    # NOTE: The last logit vector is used for predicting the next token, which is not part of the input. Therefore, we exclude it.\n",
    "    print('Discarding last logit vector...')\n",
    "    logits: Tensor = outputs.logits.data[:-1]\n",
    "    print('logits.shape: ', logits.shape)\n",
    "    # tokens: List[str] = tokenizer.convert_ids_to_tokens(inputs[1:])\n",
    "    src_tokens: List[str] = tokenizer.batch_decode([[i] for i in inputs[1:]])\n",
    "    return src_tokens, logits\n",
    "\n",
    "def get_scores(logits: Tensor) -> Tensor:\n",
    "    ret: Tensor = torch.distributions.Categorical(logits=logits).entropy()\n",
    "    print('scores.shape: ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "# def get_topk(logits: Tensor, topk: int = 5):\n",
    "#     topk_prob_values, topk_prob_inds = torch.topk(logits, k=topk, dim=1)\n",
    "#     return topk_prob_values, topk_prob_inds\n",
    "\n",
    "tokens, logits = get_src_tokens_and_logits(code_example, model_name='Salesforce/codegen-350M-mono')\n",
    "scores = get_scores(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits -= logits.min(dim=-1, keepdim=True).values\n",
    "# logits /= logits.max(dim=-1, keepdim=True).values\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_colors(scores: Tensor) -> List[str]:\n",
    "    cmap = mpl.colormaps['YlOrBr']\n",
    "    rgbas: np.ndarray = cmap(scores)\n",
    "    return np.apply_along_axis(mpl.colors.rgb2hex, -1, rgbas)\n",
    "\n",
    "def get_html(in_text: str):\n",
    "        tokens, logits = get_src_tokens_and_logits(in_text, model_name='Salesforce/codegen-350M-mono')\n",
    "        scores = get_scores(logits)\n",
    "        colors = get_colors(scores)\n",
    "        assert len(tokens) == len(colors), f'len(tokens)={len(tokens)} != len(colors)={len(colors)}'\n",
    "        ret = ''.join([f'<span style=\"background-color: {c}\">{t}</span>' for t, c in zip(tokens, colors)])\n",
    "        return f'<pre><code class=\"python\">{ret}</code></pre>'\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=get_html,\n",
    "    inputs=gr.Textbox(label='Code example', placeholder=code_example, value=code_example),\n",
    "    outputs=gr.Markdown()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n"
     ]
    }
   ],
   "source": [
    "print(code_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_96257/103305724.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><code class=\"python\"><span style=\"background-color: #662506\">#</span><span style=\"background-color: #662506\"> Hand</span><span style=\"background-color: #662506\">shake</span><span style=\"background-color: #662506\"> successful</span><span style=\"background-color: #662506\">,</span><span style=\"background-color: #662506\"> kill</span><span style=\"background-color: #662506\"> previous</span><span style=\"background-color: #662506\"> client</span><span style=\"background-color: #662506\"> if</span><span style=\"background-color: #662506\"> there</span><span style=\"background-color: #662506\"> is</span><span style=\"background-color: #662506\"> any</span><span style=\"background-color: #662506\">.</span><span style=\"background-color: #662506\">\n",
       "</span><span style=\"background-color: #662506\">with</span><span style=\"background-color: #662506\"> current</span><span style=\"background-color: #fec24d\">_</span><span style=\"background-color: #662506\">client</span><span style=\"background-color: #662506\">_</span><span style=\"background-color: #662506\">pid</span><span style=\"background-color: #662506\">.</span><span style=\"background-color: #662506\">get</span><span style=\"background-color: #fecc61\">_</span><span style=\"background-color: #662506\">lock</span><span style=\"background-color: #f17b1a\">():</span><span style=\"background-color: #ffeea9\">\n",
       "</span><span style=\"background-color: #ae3e03\">    </span><span style=\"background-color: #662506\">old</span><span style=\"background-color: #ee7617\">_</span><span style=\"background-color: #662506\">pid</span><span style=\"background-color: #fecc61\"> =</span><span style=\"background-color: #db5d0b\"> current</span><span style=\"background-color: #ffffe5\">_</span><span style=\"background-color: #fffbcf\">client</span><span style=\"background-color: #fffddc\">_</span><span style=\"background-color: #fffddd\">pid</span><span style=\"background-color: #fed26d\">.</span><span style=\"background-color: #fea736\">value</span><span style=\"background-color: #fea030\">\n",
       "</span><span style=\"background-color: #662506\">    </span><span style=\"background-color: #662506\">if</span><span style=\"background-color: #d55607\"> old</span><span style=\"background-color: #ffffe5\">_</span><span style=\"background-color: #ffffe5\">pid</span><span style=\"background-color: #662506\">!=</span><span style=\"background-color: #662506\"> 0</span><span style=\"background-color: #fec857\">:</span><span style=\"background-color: #fff7bc\">\n",
       "</span><span style=\"background-color: #fffcd6\">        </span><span style=\"background-color: #662506\">print</span><span style=\"background-color: #662506\">(</span><span style=\"background-color: #de610c\">f</span><span style=\"background-color: #882f05\">\"</span><span style=\"background-color: #662506\">B</span><span style=\"background-color: #662506\">ooting</span><span style=\"background-color: #662506\"> previous</span><span style=\"background-color: #dd5f0c\"> client</span><span style=\"background-color: #662506\"> (</span><span style=\"background-color: #662506\">pid</span><span style=\"background-color: #662506\">={</span><span style=\"background-color: #ffeea8\">old</span><span style=\"background-color: #ffffe5\">_</span><span style=\"background-color: #ffffe5\">pid</span><span style=\"background-color: #ee7416\">})</span><span style=\"background-color: #662506\">\")</span><span style=\"background-color: #fff7bd\">\n",
       "</span><span style=\"background-color: #fee390\">        </span><span style=\"background-color: #662506\">os</span><span style=\"background-color: #fffcd4\">.</span><span style=\"background-color: #feb340\">kill</span><span style=\"background-color: #fff1ae\">(</span><span style=\"background-color: #fecc61\">old</span><span style=\"background-color: #ffffe5\">_</span><span style=\"background-color: #ffffe5\">pid</span><span style=\"background-color: #fffddd\">,</span><span style=\"background-color: #8c3004\"> signal</span><span style=\"background-color: #fffcd4\">.</span><span style=\"background-color: #fffacb\">S</span><span style=\"background-color: #ffeea9\">IG</span><span style=\"background-color: #662506\">K</span><span style=\"background-color: #ffffe5\">ILL</span><span style=\"background-color: #fffcd8\">)</span><span style=\"background-color: #ee7416\">\n",
       "</span><span style=\"background-color: #662506\">        </span><span style=\"background-color: #662506\">current</span><span style=\"background-color: #ffffe5\">_</span><span style=\"background-color: #fff4b5\">client</span><span style=\"background-color: #fff6ba\">_</span><span style=\"background-color: #fff6b9\">pid</span><span style=\"background-color: #fee493\">.</span><span style=\"background-color: #fece65\">value</span><span style=\"background-color: #fff8c4\"> =</span><span style=\"background-color: #fec652\"> os</span><span style=\"background-color: #fffee2\">.</span><span style=\"background-color: #feb945\">get</span><span style=\"background-color: #fee89d\">pid</span><span style=\"background-color: #fffee2\">()</span></code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(get_html(code_example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detecting-fake-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d326f5c1b5508426345996e5494dd1240239c10c668b72b11a42af14375058c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
