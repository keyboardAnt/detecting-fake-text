{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = code_example = \\\n",
    "\"\"\"# Handshake successful, kill previous client if there is any.\n",
    "with current_client_pid.get_lock():\n",
    "    old_pid = current_client_pid.value\n",
    "    if old_pid != 0:\n",
    "        print(f\"Booting previous client (pid={old_pid})\")\n",
    "        os.kill(old_pid, signal.SIGKILL)\n",
    "        current_client_pid.value = os.getpid()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def get_tokens_and_logits(in_text: str, model_name: str, device: str = None) -> Tuple[List[str], Tensor]:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    print('Model: ', model.__class__.__name__)\n",
    "    print('Adding bos token...')\n",
    "    in_text = f'{tokenizer.bos_token}{in_text}'\n",
    "    print('Input:')\n",
    "    print(in_text)\n",
    "    print('------')\n",
    "    inputs: Tensor = tokenizer(in_text, return_tensors='pt').data['input_ids'].squeeze()\n",
    "    print(\"Input token ids' shape: \", inputs.shape)\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('Device: ', device)\n",
    "    outputs: Tensor = model(inputs.to(torch.device(device)))\n",
    "    # NOTE: The last logit vector is used for predicting the next token, which is not part of the input. Therefore, we exclude it.\n",
    "    print('Discarding last logit vector...')\n",
    "    logits: Tensor = outputs.logits.data[:-1]\n",
    "    print('logits.shape: ', logits.shape)\n",
    "    tokens: List[str] = tokenizer.decode(inputs[1:])\n",
    "    return tokens, logits\n",
    "\n",
    "def get_scores(logits: Tensor) -> Tensor:\n",
    "    ret: Tensor = torch.distributions.Categorical(logits=logits).entropy()\n",
    "    print('scores.shape: ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "# def get_topk(logits: Tensor, topk: int = 5):\n",
    "#     topk_prob_values, topk_prob_inds = torch.topk(logits, k=topk, dim=1)\n",
    "#     return topk_prob_values, topk_prob_inds\n",
    "\n",
    "tokens, logits = get_tokens_and_logits(code_example, model_name='Salesforce/codegen-350M-mono')\n",
    "scores = get_scores(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits -= logits.min(dim=-1, keepdim=True).values\n",
    "# logits /= logits.max(dim=-1, keepdim=True).values\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/routes.py\", line 384, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 1032, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 844, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_92022/1633963094.py\", line 16, in get_markdown_heatmap\n",
      "    assert len(tokens) == len(colors), f'len(tokens)={len(tokens)} != len(colors)={len(colors)}'\n",
      "AssertionError: len(tokens)=302 != len(colors)=97\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_colors(scores: Tensor) -> List[str]:\n",
    "    cmap = mpl.colormaps['YlOrBr']\n",
    "    rgbas: np.ndarray = cmap(scores)\n",
    "    return np.apply_along_axis(mpl.colors.rgb2hex, -1, rgbas)\n",
    "\n",
    "def get_markdown_heatmap(in_text: str):\n",
    "        tokens, logits = get_tokens_and_logits(in_text, model_name='Salesforce/codegen-350M-mono')\n",
    "        scores = get_scores(logits)\n",
    "        colors = get_colors(scores)\n",
    "        assert len(tokens) == len(colors), f'len(tokens)={len(tokens)} != len(colors)={len(colors)}'\n",
    "        ret = ''.join([f'<span style=\"background-color: {c}\">{t}</span>' for t, c in zip(tokens, colors)])\n",
    "        return f'```{ret}'\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\n",
    "#     \"\"\"\n",
    "#     # Demo\n",
    "#     \"\"\")\n",
    "#     inp = gr.Textbox(label='Code example', placeholder=code_example, value=code_example)\n",
    "#     tokens, logits = get_tokens_and_logits(inp, model_name='Salesforce/codegen-350M-mono')\n",
    "#     scores = get_scores(logits)\n",
    "#     colors = get_colors(scores)\n",
    "#     out = gr.Markdown(label=\"Heatmap\")\n",
    "#     btn = gr.Button(value=\"Run\")\n",
    "#     btn.click(get_markdown_heatmap, inputs=[tokens, colors], outputs=out)\n",
    "demo = gr.Interface(\n",
    "    fn=get_markdown_heatmap,\n",
    "    inputs=gr.Textbox(label='Code example', placeholder=code_example, value=code_example),\n",
    "    outputs=gr.Markdown()\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detecting-fake-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d326f5c1b5508426345996e5494dd1240239c10c668b72b11a42af14375058c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
