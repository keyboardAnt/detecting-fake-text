{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# import transformers\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# # def generate(model_name=\"Salesforce/codegen-350M-mono\", text=\"World\"):\n",
    "# #     model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# #     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# #     input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "# #     output = model.generate(input_ids, max_length=100, do_sample=True)\n",
    "# #     return tokenizer.decode(output[0])\n",
    "\n",
    "# def get_token_likelihoods(model_name=\"Salesforce/codegen-350M-mono\", text=\"World\"):\n",
    "#     # get likelihoods for each token\n",
    "#     model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "#     # start_token = self.enc(self.enc.bos_token, return_tensors='pt').data['input_ids'][0]\n",
    "#     input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "#     out = model(input_ids)\n",
    "#     probs = F.softmax(out.logits, dim=-1).squeeze()\n",
    "    \n",
    "#     output = []\n",
    "#     for tok, logits in zip(input_ids.squeeze(), probs):\n",
    "#         output.append((\n",
    "#             tokenizer.decode(tok),\n",
    "#             str(round(logits[tok].item() * 100, 4)) + \"%\",\n",
    "#             # tokenizer.decode(np.argmax(logits.detach()))\n",
    "#         ))\n",
    "\n",
    "#     return output\n",
    "\n",
    "# code_example = \\\n",
    "# \"\"\"<|endoftext|># Handshake successful, kill previous client if there is any.\n",
    "# with current_client_pid.get_lock():\n",
    "#     old_pid = current_client_pid.value\n",
    "#     if old_pid != 0:\n",
    "#         print(f\"Booting previous client (pid={old_pid})\")\n",
    "#         os.kill(old_pid, signal.SIGKILL)\n",
    "#         current_client_pid.value = os.getpid()\"\"\"\n",
    "\n",
    "# # demo = gr.Interface(\n",
    "# #     fn=get_token_likelihoods,\n",
    "# #     title=\"Heatmap Demo\",\n",
    "# #     inputs = [\n",
    "# #         gr.Textbox(\n",
    "# #             label=\"Model name\",\n",
    "# #             lines=1,\n",
    "# #             value=\"Salesforce/codegen-350M-mono\",\n",
    "# #         ),\n",
    "# #         gr.Textbox(\n",
    "# #             label=\"Text\",\n",
    "# #             lines=8,\n",
    "# #             value=code_example,\n",
    "# #         ),\n",
    "# #     ],\n",
    "# #     # outputs = gr.HighlightedText(\n",
    "# #     #     label=\"Probs\",\n",
    "# #     # ).style(color_map={\"+\": \"red\", \"-\": \"green\"}),\n",
    "# #     outputs = gr.Markdown()\n",
    "# # )\n",
    "\n",
    "# def welcome(code_example):\n",
    "#     # return f\"Welcome to Gradio, {name}!\"\n",
    "#     return get_token_likelihoods(text=code_example)\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     inp = gr.Textbox(label='Code', placeholder=code_example, value=code_example)\n",
    "#     out = gr.Textbox(label='Heatmap')\n",
    "#     inp.change(welcome, inp, out)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: Salesforce/codegen-350M-multi\n"
     ]
    }
   ],
   "source": [
    "# import gradio as gr\n",
    "# # import transformers\n",
    "# # import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# from transformers import (AutoModelForCausalLM, AutoTokenizer)\n",
    "# import torch\n",
    "\n",
    "\n",
    "# class AbstractLanguageChecker:\n",
    "#     \"\"\"\n",
    "#     Abstract Class that defines the Backend API of GLTR.\n",
    "\n",
    "#     To extend the GLTR interface, you need to inherit this and\n",
    "#     fill in the defined functions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         In the subclass, you need to load all necessary components\n",
    "#         for the other functions.\n",
    "#         Typically, this will comprise a tokenizer and a model.\n",
    "#         \"\"\"\n",
    "#         self.device = torch.device(\n",
    "#             \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     def check_probabilities(self, in_text, topk=40):\n",
    "#         \"\"\"\n",
    "#         Function that GLTR interacts with to check the probabilities of words\n",
    "\n",
    "#         Params:\n",
    "#         - in_text: str -- The text that you want to check\n",
    "#         - topk: int -- Your desired truncation of the head of the distribution\n",
    "\n",
    "#         Output:\n",
    "#         - payload: dict -- The wrapper for results in this function, described below\n",
    "\n",
    "#         Payload values\n",
    "#         ==============\n",
    "#         bpe_strings: list of str -- Each individual token in the text\n",
    "#         real_topk: list of tuples -- (ranking, prob) of each token\n",
    "#         pred_topk: list of list of tuple -- (word, prob) for all topk\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     def postprocess(self, token):\n",
    "#         \"\"\"\n",
    "#         clean up the tokens from any special chars and encode\n",
    "#         leading space by UTF-8 code '\\u0120', linebreak with UTF-8 code 266 '\\u010A'\n",
    "#         :param token:  str -- raw token text\n",
    "#         :return: str -- cleaned and re-encoded token text\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError\n",
    "\n",
    "\n",
    "# def top_k_logits(logits, k):\n",
    "#     \"\"\"\n",
    "#     Filters logits to only the top k choices\n",
    "#     from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "#     \"\"\"\n",
    "#     if k == 0:\n",
    "#         return logits\n",
    "#     values, _ = torch.topk(logits, k)\n",
    "#     min_values = values[:, -1]\n",
    "#     return torch.where(logits < min_values,\n",
    "#                        torch.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "#                        logits)\n",
    "\n",
    "\n",
    "# class LM(AbstractLanguageChecker):\n",
    "#     def __init__(self, model_name_or_path=\"Salesforce/codegen-350M-multi\"):\n",
    "#         super(LM, self).__init__()\n",
    "#         self.enc = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "#         self.start_token = self.enc(self.enc.bos_token, return_tensors='pt').data['input_ids'][0]\n",
    "#         print(f\"Loaded model: {model_name_or_path}\")\n",
    "\n",
    "#     def check_probabilities(self, in_text, topk=40):\n",
    "#         # Process input\n",
    "#         token_ids = self.enc(in_text, return_tensors='pt').data['input_ids'][0]\n",
    "#         token_ids = torch.concat([self.start_token, token_ids])\n",
    "#         # Forward through the model\n",
    "#         output = self.model(token_ids.to(self.device))\n",
    "#         all_logits = output.logits[:-1].detach().squeeze()\n",
    "#         # construct target and pred\n",
    "#         # yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
    "#         all_probs = torch.softmax(all_logits, dim=1)\n",
    "\n",
    "#         y = token_ids[1:]\n",
    "#         # Sort the predictions for each timestep\n",
    "#         sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()\n",
    "#         # [(pos, prob), ...]\n",
    "#         real_topk_pos = list(\n",
    "#             [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "#              for i in range(y.shape[0])])\n",
    "#         real_topk_probs = all_probs[np.arange(\n",
    "#             0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
    "#         real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
    "\n",
    "#         real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
    "#         # [str, str, ...]\n",
    "#         bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])\n",
    "\n",
    "#         bpe_strings = [self.postprocess(s) for s in bpe_strings]\n",
    "\n",
    "#         topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)\n",
    "\n",
    "#         pred_topk = [list(zip(self.enc.convert_ids_to_tokens(topk_prob_inds[i]),\n",
    "#                               topk_prob_values[i].data.cpu().numpy().tolist()\n",
    "#                               )) for i in range(y.shape[0])]\n",
    "#         pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "\n",
    "\n",
    "#         # pred_topk = []\n",
    "#         payload = {'bpe_strings': bpe_strings,\n",
    "#                    'real_topk': real_topk,\n",
    "#                    'pred_topk': pred_topk}\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         return payload\n",
    "    \n",
    "#     def postprocess(self, token):\n",
    "#         return token\n",
    "    \n",
    "# lm = LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = code_example = \\\n",
    "\"\"\"# Handshake successful, kill previous client if there is any.\n",
    "with current_client_pid.get_lock():\n",
    "    old_pid = current_client_pid.value\n",
    "    if old_pid != 0:\n",
    "        print(f\"Booting previous client (pid={old_pid})\")\n",
    "        os.kill(old_pid, signal.SIGKILL)\n",
    "        current_client_pid.value = os.getpid()\"\"\"\n",
    "\n",
    "# lm = LM()\n",
    "\n",
    "# def get(code_example):\n",
    "#     payload = lm.check_probabilities(code_example)\n",
    "#     # bpe_strings = payload['bpe_strings'][1:]\n",
    "#     # topk = payload['real_topk']\n",
    "#     return payload['pred_topk']\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     inp = gr.Textbox(label='Code example', placeholder=code_example, value=code_example)\n",
    "#     out = gr.Textbox(label='Heatmap')\n",
    "#     inp.change(get, inp, out)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Process input\n",
    "# token_ids = lm.enc(code_example, return_tensors='pt').data['input_ids'][0]\n",
    "# token_ids = torch.concat([lm.start_token, token_ids])\n",
    "# token_ids\n",
    "# # Forward through the model\n",
    "# output = lm.model(token_ids.to(lm.device))\n",
    "# output\n",
    "# all_logits = output.logits[:-1].detach().squeeze()\n",
    "# # construct target and pred\n",
    "# # yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
    "# all_probs = torch.softmax(all_logits, dim=1)\n",
    "\n",
    "# y = token_ids[1:]\n",
    "# # Sort the predictions for each timestep\n",
    "# sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()\n",
    "# # [(pos, prob), ...]\n",
    "# real_topk_pos = list(\n",
    "#     [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "#         for i in range(y.shape[0])])\n",
    "# real_topk_probs = all_probs[np.arange(\n",
    "#     0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
    "# real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
    "\n",
    "# real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
    "# # [str, str, ...]\n",
    "# bpe_strings = lm.enc.convert_ids_to_tokens(token_ids[:])\n",
    "\n",
    "# bpe_strings = [lm.postprocess(s) for s in bpe_strings]\n",
    "\n",
    "# topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)\n",
    "\n",
    "# pred_topk = [list(zip(lm.enc.convert_ids_to_tokens(topk_prob_inds[i]),\n",
    "#                         topk_prob_values[i].data.cpu().numpy().tolist()\n",
    "#                         )) for i in range(y.shape[0])]\n",
    "# pred_topk = [[(lm.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "\n",
    "\n",
    "# # pred_topk = []\n",
    "# payload = {'bpe_strings': bpe_strings,\n",
    "#             'real_topk': real_topk,\n",
    "#             'pred_topk': pred_topk}\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape:  torch.Size([97, 51200])\n"
     ]
    }
   ],
   "source": [
    "# token_ids = lm.enc(in_text, return_tensors='pt').data['input_ids'][0]\n",
    "# # token_ids = torch.concat([lm.start_token, token_ids])\n",
    "# # Forward through the model\n",
    "# output = lm.model(token_ids.to(lm.device))\n",
    "# logits = output.logits.data\n",
    "# print('logits.shape: ', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def get_tokens_and_logits(in_text: str, model_name: str, device: str = None) -> Tuple[List[str], Tensor]:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    print('Model: ', model.__class__.__name__)\n",
    "    print('Adding bos token...')\n",
    "    in_text = f'{tokenizer.bos_token}{in_text}'\n",
    "    print('Input:')\n",
    "    print(in_text)\n",
    "    print('------')\n",
    "    inputs: Tensor = tokenizer(in_text, return_tensors='pt').data['input_ids'].squeeze()\n",
    "    print(\"Input token ids' shape: \", inputs.shape)\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('Device: ', device)\n",
    "    outputs: Tensor = model(inputs.to(torch.device(device)))\n",
    "    # NOTE: The last logit vector is used for predicting the next token, which is not part of the input. Therefore, we exclude it.\n",
    "    print('Discarding last logit vector...')\n",
    "    logits: Tensor = outputs.logits.data[:-1]\n",
    "    print('logits.shape: ', logits.shape)\n",
    "    tokens: List[str] = tokenizer.decode(inputs[1:])\n",
    "    return tokens, logits\n",
    "\n",
    "def get_scores(logits: Tensor) -> Tensor:\n",
    "    ret: Tensor = torch.distributions.Categorical(logits=logits).entropy()\n",
    "    print('scores.shape: ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "# def get_topk(logits: Tensor, topk: int = 5):\n",
    "#     topk_prob_values, topk_prob_inds = torch.topk(logits, k=topk, dim=1)\n",
    "#     return topk_prob_values, topk_prob_inds\n",
    "\n",
    "tokens, logits = get_tokens_and_logits(code_example, model_name='Salesforce/codegen-350M-mono')\n",
    "scores = get_scores(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8421, 0.8407, 0.8675,  ..., 0.0477, 0.0475, 0.0475],\n",
       "        [0.6350, 0.5575, 0.5599,  ..., 0.0682, 0.0678, 0.0679],\n",
       "        [0.8095, 0.7691, 0.7231,  ..., 0.0457, 0.0455, 0.0456],\n",
       "        ...,\n",
       "        [0.4795, 0.5214, 0.6179,  ..., 0.0676, 0.0675, 0.0676],\n",
       "        [0.6694, 0.6995, 0.7520,  ..., 0.0511, 0.0512, 0.0512],\n",
       "        [0.7615, 0.7607, 0.8356,  ..., 0.0477, 0.0478, 0.0478]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits -= logits.min(dim=-1, keepdim=True).values\n",
    "logits /= logits.max(dim=-1, keepdim=True).values\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7888\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/routes.py\", line 384, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 1032, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 844, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_61988/2671309002.py\", line 16, in get_markdown_heatmap\n",
      "    assert len(tokens) == len(colors), f'len(tokens) != len(colors): {len(tokens)} != {len(colors)}'\n",
      "AssertionError: len(tokens) != len(colors): 302 != 97\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_colors(scores: Tensor) -> List[str]:\n",
    "    cmap = mpl.colormaps['YlOrBr']\n",
    "    rgbas: np.ndarray = cmap(scores)\n",
    "    return np.apply_along_axis(mpl.colors.rgb2hex, -1, rgbas)\n",
    "\n",
    "def get_markdown_heatmap(in_text: str):\n",
    "        tokens, logits = get_tokens_and_logits(in_text, model_name='Salesforce/codegen-350M-mono')\n",
    "        scores = get_scores(logits)\n",
    "        colors = get_colors(scores)\n",
    "        assert len(tokens) == len(colors)\n",
    "        ret = ''.join([f'<span style=\"background-color: {c}\">{t}</span>' for t, c in zip(tokens, colors)])\n",
    "        return f'```{ret}'\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\n",
    "#     \"\"\"\n",
    "#     # Demo\n",
    "#     \"\"\")\n",
    "#     inp = gr.Textbox(label='Code example', placeholder=code_example, value=code_example)\n",
    "#     tokens, logits = get_tokens_and_logits(inp, model_name='Salesforce/codegen-350M-mono')\n",
    "#     scores = get_scores(logits)\n",
    "#     colors = get_colors(scores)\n",
    "#     out = gr.Markdown(label=\"Heatmap\")\n",
    "#     btn = gr.Button(value=\"Run\")\n",
    "#     btn.click(get_markdown_heatmap, inputs=[tokens, colors], outputs=out)\n",
    "demo = gr.Interface(\n",
    "    fn=get_markdown_heatmap,\n",
    "    inputs=gr.Textbox(label='Code example', placeholder=code_example, value=code_example),\n",
    "    outputs=gr.Markdown()\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#662506', '#662506', '#662506', '#662506', '#662506', '#662506',\n",
       "       '#662506', '#662506', '#662506', '#662506', '#662506', '#662506',\n",
       "       '#662506', '#662506', '#662506', '#662506', '#fec24d', '#662506',\n",
       "       '#662506', '#662506', '#662506', '#662506', '#fecc61', '#662506',\n",
       "       '#f17b1a', '#ffeea9', '#ae3e03', '#662506', '#ee7617', '#662506',\n",
       "       '#fecc61', '#db5d0b', '#ffffe5', '#fffbcf', '#fffddc', '#fffddd',\n",
       "       '#fed26d', '#fea736', '#fea030', '#662506', '#662506', '#d55607',\n",
       "       '#ffffe5', '#ffffe5', '#662506', '#662506', '#fec857', '#fff7bc',\n",
       "       '#fffcd6', '#662506', '#662506', '#de610c', '#882f05', '#662506',\n",
       "       '#662506', '#662506', '#dd5f0c', '#662506', '#662506', '#662506',\n",
       "       '#ffeea8', '#ffffe5', '#ffffe5', '#ee7416', '#662506', '#fff7bd',\n",
       "       '#fee390', '#662506', '#fffcd4', '#feb340', '#fff1ae', '#fecc61',\n",
       "       '#ffffe5', '#ffffe5', '#fffddd', '#8c3004', '#fffcd4', '#fffacb',\n",
       "       '#ffeea9', '#662506', '#ffffe5', '#fffcd8', '#ee7416', '#662506',\n",
       "       '#662506', '#ffffe5', '#fff4b5', '#fff6ba', '#fff6b9', '#fee493',\n",
       "       '#fece65', '#fff8c4', '#fec652', '#fffee2', '#feb945', '#fee89d',\n",
       "       '#fffee2'], dtype='<U7')"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_colors(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detecting-fake-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d326f5c1b5508426345996e5494dd1240239c10c668b72b11a42af14375058c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
