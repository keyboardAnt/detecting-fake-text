{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = code_example = \\\n",
    "\"\"\"# Handshake successful, kill previous client if there is any.\n",
    "with current_client_pid.get_lock():\n",
    "    old_pid = current_client_pid.value\n",
    "    if old_pid != 0:\n",
    "        print(f\"Booting previous client (pid={old_pid})\")\n",
    "        os.kill(old_pid, signal.SIGKILL)\n",
    "        current_client_pid.value = os.getpid()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def get_tokens_and_logits(in_text: str, model_name: str, device: str = None) -> Tuple[List[str], Tensor]:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    print('Model: ', model.__class__.__name__)\n",
    "    print('Adding bos token...')\n",
    "    in_text = f'{tokenizer.bos_token}{in_text}'\n",
    "    print('Input:')\n",
    "    print(in_text)\n",
    "    print('------')\n",
    "    inputs: Tensor = tokenizer(in_text, return_tensors='pt').data['input_ids'].squeeze()\n",
    "    print(\"Input token ids' shape: \", inputs.shape)\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('Device: ', device)\n",
    "    outputs: Tensor = model(inputs.to(torch.device(device)))\n",
    "    # NOTE: The last logit vector is used for predicting the next token, which is not part of the input. Therefore, we exclude it.\n",
    "    print('Discarding last logit vector...')\n",
    "    logits: Tensor = outputs.logits.data[:-1]\n",
    "    print('logits.shape: ', logits.shape)\n",
    "    tokens: List[str] = tokenizer.decode(inputs[1:])\n",
    "    return tokens, logits\n",
    "\n",
    "def get_scores(logits: Tensor) -> Tensor:\n",
    "    ret: Tensor = torch.distributions.Categorical(logits=logits).entropy()\n",
    "    print('scores.shape: ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "# def get_topk(logits: Tensor, topk: int = 5):\n",
    "#     topk_prob_values, topk_prob_inds = torch.topk(logits, k=topk, dim=1)\n",
    "#     return topk_prob_values, topk_prob_inds\n",
    "\n",
    "tokens, logits = get_tokens_and_logits(code_example, model_name='Salesforce/codegen-350M-mono')\n",
    "scores = get_scores(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8421, 0.8407, 0.8675,  ..., 0.0477, 0.0475, 0.0475],\n",
       "        [0.6350, 0.5575, 0.5599,  ..., 0.0682, 0.0678, 0.0679],\n",
       "        [0.8095, 0.7691, 0.7231,  ..., 0.0457, 0.0455, 0.0456],\n",
       "        ...,\n",
       "        [0.4795, 0.5214, 0.6179,  ..., 0.0676, 0.0675, 0.0676],\n",
       "        [0.6694, 0.6995, 0.7520,  ..., 0.0511, 0.0512, 0.0512],\n",
       "        [0.7615, 0.7607, 0.8356,  ..., 0.0477, 0.0478, 0.0478]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits -= logits.min(dim=-1, keepdim=True).values\n",
    "# logits /= logits.max(dim=-1, keepdim=True).values\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7888\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CodeGenForCausalLM\n",
      "Adding bos token...\n",
      "Input:\n",
      "<|endoftext|># Handshake successful, kill previous client if there is any.\n",
      "with current_client_pid.get_lock():\n",
      "    old_pid = current_client_pid.value\n",
      "    if old_pid != 0:\n",
      "        print(f\"Booting previous client (pid={old_pid})\")\n",
      "        os.kill(old_pid, signal.SIGKILL)\n",
      "        current_client_pid.value = os.getpid()\n",
      "------\n",
      "Input token ids' shape:  torch.Size([98])\n",
      "Device:  cpu\n",
      "Discarding last logit vector...\n",
      "logits.shape:  torch.Size([97, 51200])\n",
      "scores.shape:  torch.Size([97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/routes.py\", line 384, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 1032, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/gradio/blocks.py\", line 844, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/nadavt/opt/anaconda3/envs/detecting-fake-text/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_61988/2671309002.py\", line 16, in get_markdown_heatmap\n",
      "    assert len(tokens) == len(colors), f'len(tokens) != len(colors): {len(tokens)} != {len(colors)}'\n",
      "AssertionError: len(tokens) != len(colors): 302 != 97\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_colors(scores: Tensor) -> List[str]:\n",
    "    cmap = mpl.colormaps['YlOrBr']\n",
    "    rgbas: np.ndarray = cmap(scores)\n",
    "    return np.apply_along_axis(mpl.colors.rgb2hex, -1, rgbas)\n",
    "\n",
    "def get_markdown_heatmap(in_text: str):\n",
    "        tokens, logits = get_tokens_and_logits(in_text, model_name='Salesforce/codegen-350M-mono')\n",
    "        scores = get_scores(logits)\n",
    "        colors = get_colors(scores)\n",
    "        assert len(tokens) == len(colors)\n",
    "        ret = ''.join([f'<span style=\"background-color: {c}\">{t}</span>' for t, c in zip(tokens, colors)])\n",
    "        return f'```{ret}'\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\n",
    "#     \"\"\"\n",
    "#     # Demo\n",
    "#     \"\"\")\n",
    "#     inp = gr.Textbox(label='Code example', placeholder=code_example, value=code_example)\n",
    "#     tokens, logits = get_tokens_and_logits(inp, model_name='Salesforce/codegen-350M-mono')\n",
    "#     scores = get_scores(logits)\n",
    "#     colors = get_colors(scores)\n",
    "#     out = gr.Markdown(label=\"Heatmap\")\n",
    "#     btn = gr.Button(value=\"Run\")\n",
    "#     btn.click(get_markdown_heatmap, inputs=[tokens, colors], outputs=out)\n",
    "demo = gr.Interface(\n",
    "    fn=get_markdown_heatmap,\n",
    "    inputs=gr.Textbox(label='Code example', placeholder=code_example, value=code_example),\n",
    "    outputs=gr.Markdown()\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detecting-fake-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d326f5c1b5508426345996e5494dd1240239c10c668b72b11a42af14375058c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
